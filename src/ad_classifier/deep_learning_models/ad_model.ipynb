{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NorzUtvsSZPG"
      },
      "source": [
        "# Set Up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rgKkuEKSUYl",
        "outputId": "4fc4fd39-1759-4c62-ba7e-b42005c9eec8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Get Dataset\n",
        "with zipfile.ZipFile(\"/content/drive/MyDrive/ADNI_TRAINING_DATASET.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"/content/dataset\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIQXX-L6KDjX"
      },
      "source": [
        "## Define Config\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAD3s1AtihY3"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ALzkpBSJJ__F"
      },
      "outputs": [],
      "source": [
        "IMAGE_SIZE = [200, 200]\n",
        "CLASS_NAMES = [\"AD\", \"CN\", \"MCI\", \"pMCI\"]\n",
        "\n",
        "MODEL_SAVE_PATH = \"/content/drive/MyDrive/models\"\n",
        "\n",
        "DATASET_PATH = \"/content/dataset/ADNI_TRAINING_DATASET\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hek1heu_Sq-a"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QXAqgCV1Sr8Q"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "from datetime import datetime\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lvxFer9F_tZ"
      },
      "source": [
        "# Custom Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FOJwYGk_F-_L"
      },
      "outputs": [],
      "source": [
        "#------------------------------------COGNINET-------------------------------#\n",
        "\n",
        "def convolutional_block(filters):\n",
        "    return tf.keras.Sequential([\n",
        "        tf.keras.layers.SeparableConv2D(\n",
        "            filters, 3, activation='relu', padding='same'),\n",
        "        tf.keras.layers.SeparableConv2D(\n",
        "            filters, 3, activation='relu', padding='same'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.AveragePooling2D()\n",
        "    ]\n",
        "    )\n",
        "\n",
        "\n",
        "def dense_block(units, dropout_rate):\n",
        "    return tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(units, activation='relu'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Dropout(dropout_rate)\n",
        "    ])\n",
        "\n",
        "def cogni_net():\n",
        "    model = tf.keras.Sequential([\n",
        "            tf.keras.Input(shape=(*IMAGE_SIZE, 1)),\n",
        "\n",
        "            tf.keras.layers.Conv2D(16, 3, activation='relu', padding='same'),\n",
        "            tf.keras.layers.Conv2D(16, 3, activation='relu', padding='same'),\n",
        "            tf.keras.layers.AveragePooling2D(),\n",
        "\n",
        "            convolutional_block(32),\n",
        "            convolutional_block(64),\n",
        "\n",
        "            convolutional_block(128),\n",
        "            tf.keras.layers.Dropout(0.2),\n",
        "\n",
        "            convolutional_block(256),\n",
        "            tf.keras.layers.Dropout(0.2),\n",
        "\n",
        "            tf.keras.layers.Flatten(),\n",
        "            dense_block(512, 0.7),\n",
        "            dense_block(128, 0.5),\n",
        "            dense_block(64, 0.3),\n",
        "\n",
        "            tf.keras.layers.Dense(\n",
        "                4, activation='softmax')\n",
        "        ])\n",
        "    return model\n",
        "\n",
        "def alternative_cogni_net_1():\n",
        "    model = tf.keras.Sequential([\n",
        "            tf.keras.Input(shape=(*IMAGE_SIZE, 1)),\n",
        "\n",
        "            tf.keras.layers.Conv2D(16, 3, activation='relu', padding='same'),\n",
        "            tf.keras.layers.Conv2D(16, 3, activation='relu', padding='same'),\n",
        "            tf.keras.layers.AveragePooling2D(),\n",
        "\n",
        "            convolutional_block(32),\n",
        "            convolutional_block(64),\n",
        "\n",
        "            convolutional_block(128),\n",
        "            tf.keras.layers.Dropout(0.2),\n",
        "\n",
        "            convolutional_block(256),\n",
        "            tf.keras.layers.Dropout(0.2),\n",
        "\n",
        "            convolutional_block(512),\n",
        "            tf.keras.layers.Dropout(0.2),\n",
        "\n",
        "            tf.keras.layers.Flatten(),\n",
        "            dense_block(128, 0.5),\n",
        "            dense_block(64, 0.3),\n",
        "\n",
        "            tf.keras.layers.Dense(\n",
        "                4, activation='softmax')\n",
        "        ])\n",
        "    return model\n",
        "\n",
        "def alternative_cogni_net_2():\n",
        "    model = tf.keras.Sequential([\n",
        "            tf.keras.Input(shape=(*IMAGE_SIZE, 1)),\n",
        "\n",
        "            tf.keras.layers.Conv2D(16, 3, activation='relu', padding='same'),\n",
        "            tf.keras.layers.Conv2D(16, 3, activation='relu', padding='same'),\n",
        "            tf.keras.layers.AveragePooling2D(),\n",
        "\n",
        "            convolutional_block(32),\n",
        "\n",
        "            convolutional_block(64),\n",
        "            tf.keras.layers.Dropout(0.2),\n",
        "\n",
        "            tf.keras.layers.Flatten(),\n",
        "            dense_block(256, 0.7),\n",
        "            dense_block(128, 0.5),\n",
        "            dense_block(64, 0.3),\n",
        "\n",
        "            tf.keras.layers.Dense(\n",
        "                4, activation='softmax')\n",
        "        ])\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdTl5O2uFyu-"
      },
      "source": [
        "# Models Using Tensorflow Applications"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4R3bb016Fyu_"
      },
      "outputs": [],
      "source": [
        "def get_full_model(base_model):\n",
        "    x = tf.keras.layers.Flatten()(base_model.output)\n",
        "    x = tf.keras.layers.Dense(1024, activation='relu')(x)\n",
        "    x = tf.keras.layers.Dropout(0.2)(x)\n",
        "\n",
        "    # Add a final sigmoid layer with 1 node for classification output\n",
        "    x = tf.keras.layers.Dense(4, activation='softmax')(x)\n",
        "\n",
        "    return tf.keras.models.Model(base_model.input, x)\n",
        "\n",
        "# ------------------------------------DenseNet-------------------------------#\n",
        "\n",
        "def DenseNet201():\n",
        "    base_model = tf.keras.applications.densenet.DenseNet201(\n",
        "        include_top=False,\n",
        "        weights=None,\n",
        "        input_shape=(*IMAGE_SIZE, 1),\n",
        "        pooling=\"avg\",\n",
        "        classes=4,\n",
        "        classifier_activation='softmax'\n",
        "    )\n",
        "\n",
        "    return get_full_model(base_model)\n",
        "\n",
        "# ------------------------------------Inception V3-------------------------------#\n",
        "\n",
        "def InceptionV3():\n",
        "    base_model = tf.keras.applications.inception_v3.InceptionV3(\n",
        "        include_top=False,\n",
        "        weights=None,\n",
        "        input_shape=(*IMAGE_SIZE, 1),\n",
        "        pooling=\"avg\",\n",
        "        classes=4,\n",
        "        classifier_activation='softmax'\n",
        "    )\n",
        "\n",
        "    return get_full_model(base_model)\n",
        "\n",
        "# ------------------------------------ResNet-------------------------------#\n",
        "\n",
        "\n",
        "def ResNet50():\n",
        "    base_model = tf.keras.applications.resnet50.ResNet50(\n",
        "        include_top=False,\n",
        "        weights=None,\n",
        "        input_shape=(*IMAGE_SIZE, 1),\n",
        "        pooling=\"avg\",\n",
        "        classes=4,\n",
        "        classifier_activation='softmax'\n",
        "    )\n",
        "\n",
        "    return get_full_model(base_model)\n",
        "\n",
        "\n",
        "# ------------------------------------VGGNet-------------------------------#\n",
        "\n",
        "def VGGNet19():\n",
        "    base_model = tf.keras.applications.vgg19.VGG19(\n",
        "        include_top=False,\n",
        "        weights=None,\n",
        "        input_shape=(*IMAGE_SIZE, 1),\n",
        "        pooling=\"avg\",\n",
        "        classes=4,\n",
        "        classifier_activation='softmax'\n",
        "    )\n",
        "\n",
        "    return get_full_model(base_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ds7HNUkoKrXO"
      },
      "source": [
        "# Main Model Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMUaBqDjKtGq",
        "outputId": "8e9ea8a0-4cee-4434-93c2-692adc3f4fcb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "REPLICAS:  1\n",
            "Found 28800 files belonging to 4 classes.\n",
            "Using 23040 files for training.\n",
            "Found 28800 files belonging to 4 classes.\n",
            "Using 5760 files for validation.\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 200, 200, 16)      160       \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 200, 200, 16)      2320      \n",
            "                                                                 \n",
            " average_pooling2d (Average  (None, 100, 100, 16)      0         \n",
            " Pooling2D)                                                      \n",
            "                                                                 \n",
            " sequential (Sequential)     (None, 50, 50, 32)        2160      \n",
            "                                                                 \n",
            " sequential_1 (Sequential)   (None, 25, 25, 64)        7392      \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 25, 25, 64)        0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 40000)             0         \n",
            "                                                                 \n",
            " sequential_2 (Sequential)   (None, 256)               10241280  \n",
            "                                                                 \n",
            " sequential_3 (Sequential)   (None, 128)               33408     \n",
            "                                                                 \n",
            " sequential_4 (Sequential)   (None, 64)                8512      \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 4)                 260       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 10295492 (39.27 MB)\n",
            "Trainable params: 10294404 (39.27 MB)\n",
            "Non-trainable params: 1088 (4.25 KB)\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "719/720 [============================>.] - ETA: 0s - loss: 1.4849 - accuracy: 0.3270"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "720/720 [==============================] - 56s 65ms/step - loss: 1.4847 - accuracy: 0.3270 - val_loss: 1.3240 - val_accuracy: 0.3491 - lr: 0.0010\n",
            "Epoch 2/50\n",
            "720/720 [==============================] - 50s 69ms/step - loss: 1.2486 - accuracy: 0.4082 - val_loss: 1.1449 - val_accuracy: 0.4967 - lr: 0.0010\n",
            "Epoch 3/50\n",
            "720/720 [==============================] - 46s 64ms/step - loss: 1.1441 - accuracy: 0.4880 - val_loss: 1.1849 - val_accuracy: 0.4668 - lr: 0.0010\n",
            "Epoch 4/50\n",
            "720/720 [==============================] - 46s 63ms/step - loss: 1.0317 - accuracy: 0.5602 - val_loss: 0.9429 - val_accuracy: 0.5983 - lr: 0.0010\n",
            "Epoch 5/50\n",
            "720/720 [==============================] - 46s 64ms/step - loss: 0.9296 - accuracy: 0.6230 - val_loss: 0.8059 - val_accuracy: 0.6778 - lr: 0.0010\n",
            "Epoch 6/50\n",
            "720/720 [==============================] - 50s 70ms/step - loss: 0.8343 - accuracy: 0.6706 - val_loss: 0.6794 - val_accuracy: 0.7337 - lr: 0.0010\n",
            "Epoch 7/50\n",
            "720/720 [==============================] - 46s 64ms/step - loss: 0.7431 - accuracy: 0.7159 - val_loss: 0.6627 - val_accuracy: 0.7457 - lr: 0.0010\n",
            "Epoch 8/50\n",
            "720/720 [==============================] - 47s 65ms/step - loss: 0.6783 - accuracy: 0.7422 - val_loss: 0.6115 - val_accuracy: 0.7658 - lr: 0.0010\n",
            "Epoch 9/50\n",
            "720/720 [==============================] - 45s 62ms/step - loss: 0.6307 - accuracy: 0.7619 - val_loss: 0.6836 - val_accuracy: 0.7413 - lr: 0.0010\n",
            "Epoch 10/50\n",
            "720/720 [==============================] - 47s 65ms/step - loss: 0.5708 - accuracy: 0.7891 - val_loss: 0.5092 - val_accuracy: 0.8095 - lr: 0.0010\n",
            "Epoch 11/50\n",
            "720/720 [==============================] - 48s 66ms/step - loss: 0.5170 - accuracy: 0.8120 - val_loss: 0.7649 - val_accuracy: 0.7266 - lr: 0.0010\n",
            "Epoch 12/50\n",
            "720/720 [==============================] - 49s 68ms/step - loss: 0.4832 - accuracy: 0.8254 - val_loss: 0.5514 - val_accuracy: 0.7939 - lr: 0.0010\n",
            "Epoch 13/50\n",
            "720/720 [==============================] - 46s 63ms/step - loss: 0.4591 - accuracy: 0.8348 - val_loss: 0.5014 - val_accuracy: 0.8196 - lr: 0.0010\n",
            "Epoch 14/50\n",
            "720/720 [==============================] - 50s 69ms/step - loss: 0.4312 - accuracy: 0.8442 - val_loss: 0.4982 - val_accuracy: 0.8168 - lr: 0.0010\n",
            "Epoch 15/50\n",
            "720/720 [==============================] - 45s 63ms/step - loss: 0.4084 - accuracy: 0.8549 - val_loss: 0.4334 - val_accuracy: 0.8365 - lr: 0.0010\n",
            "Epoch 16/50\n",
            "720/720 [==============================] - 51s 70ms/step - loss: 0.3794 - accuracy: 0.8658 - val_loss: 0.3868 - val_accuracy: 0.8635 - lr: 0.0010\n",
            "Epoch 17/50\n",
            "720/720 [==============================] - 50s 69ms/step - loss: 0.3551 - accuracy: 0.8746 - val_loss: 0.4032 - val_accuracy: 0.8535 - lr: 0.0010\n",
            "Epoch 18/50\n",
            "720/720 [==============================] - 46s 63ms/step - loss: 0.3419 - accuracy: 0.8803 - val_loss: 0.3654 - val_accuracy: 0.8701 - lr: 0.0010\n",
            "Epoch 19/50\n",
            "720/720 [==============================] - 46s 63ms/step - loss: 0.3324 - accuracy: 0.8823 - val_loss: 0.5186 - val_accuracy: 0.8097 - lr: 0.0010\n",
            "Epoch 20/50\n",
            "720/720 [==============================] - 47s 65ms/step - loss: 0.3130 - accuracy: 0.8898 - val_loss: 0.3472 - val_accuracy: 0.8816 - lr: 0.0010\n",
            "Epoch 21/50\n",
            "720/720 [==============================] - 45s 62ms/step - loss: 0.2969 - accuracy: 0.8970 - val_loss: 0.4005 - val_accuracy: 0.8627 - lr: 0.0010\n",
            "Epoch 22/50\n",
            "720/720 [==============================] - 46s 64ms/step - loss: 0.2925 - accuracy: 0.8996 - val_loss: 0.3377 - val_accuracy: 0.8849 - lr: 0.0010\n",
            "Epoch 23/50\n",
            "720/720 [==============================] - 51s 70ms/step - loss: 0.2812 - accuracy: 0.9020 - val_loss: 0.3346 - val_accuracy: 0.8835 - lr: 0.0010\n",
            "Epoch 24/50\n",
            "720/720 [==============================] - 45s 62ms/step - loss: 0.2675 - accuracy: 0.9070 - val_loss: 0.3668 - val_accuracy: 0.8726 - lr: 0.0010\n",
            "Epoch 25/50\n",
            "720/720 [==============================] - 49s 68ms/step - loss: 0.2697 - accuracy: 0.9062 - val_loss: 0.3604 - val_accuracy: 0.8839 - lr: 0.0010\n",
            "Epoch 26/50\n",
            "720/720 [==============================] - 51s 70ms/step - loss: 0.2532 - accuracy: 0.9130 - val_loss: 0.3177 - val_accuracy: 0.8877 - lr: 0.0010\n",
            "Epoch 27/50\n",
            "720/720 [==============================] - 49s 68ms/step - loss: 0.2442 - accuracy: 0.9163 - val_loss: 0.3497 - val_accuracy: 0.8903 - lr: 0.0010\n",
            "Epoch 28/50\n",
            "720/720 [==============================] - 43s 59ms/step - loss: 0.2395 - accuracy: 0.9190 - val_loss: 0.3346 - val_accuracy: 0.8844 - lr: 0.0010\n",
            "Epoch 29/50\n",
            "720/720 [==============================] - 44s 61ms/step - loss: 0.2398 - accuracy: 0.9161 - val_loss: 0.3000 - val_accuracy: 0.8981 - lr: 0.0010\n",
            "Epoch 30/50\n",
            "720/720 [==============================] - 47s 65ms/step - loss: 0.2202 - accuracy: 0.9227 - val_loss: 0.3167 - val_accuracy: 0.8908 - lr: 0.0010\n",
            "Epoch 31/50\n",
            "720/720 [==============================] - 42s 58ms/step - loss: 0.2190 - accuracy: 0.9249 - val_loss: 0.4268 - val_accuracy: 0.8604 - lr: 0.0010\n",
            "Epoch 32/50\n",
            "720/720 [==============================] - 48s 66ms/step - loss: 0.2126 - accuracy: 0.9263 - val_loss: 0.3349 - val_accuracy: 0.8899 - lr: 0.0010\n",
            "Epoch 33/50\n",
            "720/720 [==============================] - 47s 65ms/step - loss: 0.2060 - accuracy: 0.9283 - val_loss: 0.3231 - val_accuracy: 0.8922 - lr: 0.0010\n",
            "Epoch 34/50\n",
            "720/720 [==============================] - 47s 64ms/step - loss: 0.1988 - accuracy: 0.9336 - val_loss: 0.3880 - val_accuracy: 0.8776 - lr: 0.0010\n",
            "Epoch 35/50\n",
            "720/720 [==============================] - 47s 66ms/step - loss: 0.1883 - accuracy: 0.9365 - val_loss: 0.3607 - val_accuracy: 0.8821 - lr: 0.0010\n",
            "Epoch 36/50\n",
            "720/720 [==============================] - 47s 65ms/step - loss: 0.1910 - accuracy: 0.9357 - val_loss: 0.3327 - val_accuracy: 0.8938 - lr: 0.0010\n",
            "Epoch 37/50\n",
            "720/720 [==============================] - 45s 62ms/step - loss: 0.1842 - accuracy: 0.9374 - val_loss: 0.2916 - val_accuracy: 0.8995 - lr: 0.0010\n",
            "Epoch 38/50\n",
            "720/720 [==============================] - 43s 59ms/step - loss: 0.1917 - accuracy: 0.9354 - val_loss: 0.3787 - val_accuracy: 0.8792 - lr: 0.0010\n",
            "Epoch 39/50\n",
            "720/720 [==============================] - 48s 67ms/step - loss: 0.1772 - accuracy: 0.9401 - val_loss: 0.2863 - val_accuracy: 0.9043 - lr: 0.0010\n",
            "Epoch 40/50\n",
            "720/720 [==============================] - 43s 60ms/step - loss: 0.1747 - accuracy: 0.9414 - val_loss: 0.3736 - val_accuracy: 0.8875 - lr: 0.0010\n",
            "Epoch 41/50\n",
            "720/720 [==============================] - 46s 63ms/step - loss: 0.1771 - accuracy: 0.9396 - val_loss: 0.3194 - val_accuracy: 0.9047 - lr: 0.0010\n",
            "Epoch 42/50\n",
            "720/720 [==============================] - 49s 68ms/step - loss: 0.1805 - accuracy: 0.9404 - val_loss: 0.2714 - val_accuracy: 0.9125 - lr: 0.0010\n",
            "Epoch 43/50\n",
            "720/720 [==============================] - 47s 65ms/step - loss: 0.1714 - accuracy: 0.9427 - val_loss: 0.3222 - val_accuracy: 0.8962 - lr: 0.0010\n",
            "Epoch 44/50\n",
            "720/720 [==============================] - 44s 61ms/step - loss: 0.1676 - accuracy: 0.9433 - val_loss: 0.2885 - val_accuracy: 0.9146 - lr: 0.0010\n",
            "Epoch 45/50\n",
            "720/720 [==============================] - 43s 59ms/step - loss: 0.1745 - accuracy: 0.9404 - val_loss: 0.3026 - val_accuracy: 0.9073 - lr: 0.0010\n",
            "Epoch 46/50\n",
            "720/720 [==============================] - 43s 59ms/step - loss: 0.1622 - accuracy: 0.9448 - val_loss: 0.2775 - val_accuracy: 0.9155 - lr: 0.0010\n",
            "Epoch 47/50\n",
            "720/720 [==============================] - 44s 61ms/step - loss: 0.1588 - accuracy: 0.9469 - val_loss: 0.2712 - val_accuracy: 0.9182 - lr: 0.0010\n",
            "Epoch 48/50\n",
            "720/720 [==============================] - 47s 65ms/step - loss: 0.1595 - accuracy: 0.9464 - val_loss: 0.2919 - val_accuracy: 0.9061 - lr: 0.0010\n",
            "Epoch 49/50\n",
            "720/720 [==============================] - 44s 61ms/step - loss: 0.1535 - accuracy: 0.9485 - val_loss: 0.2834 - val_accuracy: 0.9161 - lr: 0.0010\n",
            "Epoch 50/50\n",
            "720/720 [==============================] - 44s 61ms/step - loss: 0.1561 - accuracy: 0.9502 - val_loss: 0.2827 - val_accuracy: 0.9090 - lr: 0.0010\n"
          ]
        }
      ],
      "source": [
        "STRATEGY = tf.distribute.get_strategy()\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "BATCH_SIZE = 16 * STRATEGY.num_replicas_in_sync\n",
        "EPOCHS = 50\n",
        "\n",
        "MODEL_NAME = \"DenseNet201\"\n",
        "\n",
        "\n",
        "base_dir = f\"{MODEL_SAVE_PATH}/{MODEL_NAME}-{datetime.now().strftime('%Y-%m-%d-%H:%M')}\"\n",
        "os.makedirs(base_dir, exist_ok=True)\n",
        "\n",
        "print(\"REPLICAS: \", STRATEGY.num_replicas_in_sync)\n",
        "\n",
        "train_ds = image_dataset_from_directory(\n",
        "    DATASET_PATH,\n",
        "    labels=\"inferred\",\n",
        "    label_mode=\"categorical\",\n",
        "    image_size=IMAGE_SIZE,\n",
        "    color_mode=\"grayscale\",\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=1337\n",
        ").prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "validation_ds = image_dataset_from_directory(\n",
        "    DATASET_PATH,\n",
        "    labels=\"inferred\",\n",
        "    label_mode=\"categorical\",\n",
        "    image_size=IMAGE_SIZE,\n",
        "    color_mode=\"grayscale\",\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=1337\n",
        ").prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "with STRATEGY.scope():\n",
        "    model = DenseNet201()\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# Defining Callbacks\n",
        "save_best = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=f\"{base_dir}/model.h5\", monitor='val_loss', save_best_only=True)\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
        "    patience=5, monitor='val_accuracy', factor=0.6, min_lr=0.0000001)\n",
        "\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=validation_ds,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=[save_best, reduce_lr]\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
